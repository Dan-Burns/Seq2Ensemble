{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dan-Burns/Seq2Ensemble/blob/least_squares/ColabOpenAWSEM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQVQc0XRhkXp"
      },
      "source": [
        "### Setup the software"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPEx4U0wezNX",
        "outputId": "6de3b9e5-078d-44a0-8735-8158352c5112",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏬ Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:17\n",
            "🔁 Restarting kernel...\n",
            "Installing Python packages.\n",
            "Installing OpenAwsem\n",
            "Cloning into '$'...\n",
            "remote: Enumerating objects: 2275, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 2275 (delta 44), reused 60 (delta 18), pack-reused 2169\u001b[K\n",
            "Receiving objects: 100% (2275/2275), 60.81 MiB | 17.45 MiB/s, done.\n",
            "Resolving deltas: 100% (1009/1009), done.\n",
            "Updating files: 100% (1051/1051), done.\n",
            "--2023-03-29 18:58:17--  ftp://ftp.wwpdb.org/pub/pdb/derived_data/pdb_seqres.txt\n",
            "           => ‘pdb_seqres.txt’\n",
            "Resolving ftp.wwpdb.org (ftp.wwpdb.org)... 132.249.213.110\n",
            "Connecting to ftp.wwpdb.org (ftp.wwpdb.org)|132.249.213.110|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /pub/pdb/derived_data ... done.\n",
            "==> SIZE pdb_seqres.txt ... 261316114\n",
            "==> PASV ... done.    ==> RETR pdb_seqres.txt ... done.\n",
            "Length: 261316114 (249M) (unauthoritative)\n",
            "\n",
            "pdb_seqres.txt      100%[===================>] 249.21M  17.2MB/s    in 16s     \n",
            "\n",
            "2023-03-29 18:58:36 (15.6 MB/s) - ‘pdb_seqres.txt’ saved [261316114]\n",
            "\n",
            "--2023-03-29 18:58:36--  http://$/\n",
            "Resolving $ ($)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘$’\n",
            "FINISHED --2023-03-29 18:58:36--\n",
            "Total wall clock time: 18s\n",
            "Downloaded: 1 files, 249M in 16s (15.6 MB/s)\n",
            "mv: cannot stat 'openawsem/': No such file or directory\n",
            "--2023-03-29 18:58:36--  http://webclu.bio.wzw.tum.de/stride/stride.tar.gz\n",
            "Resolving webclu.bio.wzw.tum.de (webclu.bio.wzw.tum.de)... 141.40.43.212\n",
            "Connecting to webclu.bio.wzw.tum.de (webclu.bio.wzw.tum.de)|141.40.43.212|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 318997 (312K) [application/x-gzip]\n",
            "Saving to: ‘stride.tar.gz’\n",
            "\n",
            "stride.tar.gz       100%[===================>] 311.52K   216KB/s    in 1.4s    \n",
            "\n",
            "2023-03-29 18:58:38 (216 KB/s) - ‘stride.tar.gz’ saved [318997/318997]\n",
            "\n",
            "--2023-03-29 18:58:38--  http://$/\n",
            "Resolving $ ($)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘$’\n",
            "FINISHED --2023-03-29 18:58:38--\n",
            "Total wall clock time: 2.2s\n",
            "Downloaded: 1 files, 312K in 1.4s (216 KB/s)\n",
            "tar: $: Not found in archive\n",
            "tar: Exiting with failure status due to previous errors\n",
            "make: *** No rule to make target '$'.  Stop.\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to set everything up. This can take several minutes.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install() \n",
        "!git clone https://github.com/Dan-Burns/molecular_dynamics_analysis_tools.git &> /dev/null\n",
        "\n",
        "\n",
        "#1. Install OpenMM and core dependencies\n",
        "#!conda install -c conda-forge openmm pdbfixer mdtraj biopython py3dmol numpy pandas matplotlib\n",
        "print(\"Installing Python packages.\")\n",
        "!conda env update --file molecular_dynamics_analysis_tools/seq_to_ensemble_environment.yml --prune &> /dev/null\n",
        "#2. Download openawsem\n",
        "print(\"Installing OpenAwsem\")\n",
        "!git clone https://github.com/PotoyanGroup/openawsem &> /dev/null\n",
        "\n",
        "#3. Download pdb_seqres\n",
        "!wget ftp://ftp.wwpdb.org/pub/pdb/derived_data/pdb_seqres.txt &> /dev/null\n",
        "!mv pdb_seqres.txt openawsem/ &> /dev/null\n",
        "\n",
        "#4. Download and isntall stride\n",
        "!wget http://webclu.bio.wzw.tum.de/stride/stride.tar.gz &> /dev/null\n",
        "!mkdir stride_loc && tar -xf stride.tar.gz -C ./stride_loc &> /dev/null\n",
        "!cd stride_loc && make &> /dev/null\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftXm1usFt7lK"
      },
      "source": [
        "### Create Project and Run simulation\n",
        "\n",
        "Upload a PDB file of protein which can be obtained either via AlphaFold or from PDB database. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "Yop0UegFlWAB",
        "outputId": "62619495-c0ff-496c-fbe4-d8101f433bf6",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1faf7efb-8124-47f2-be8a-5ee03e2a9766\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1faf7efb-8124-47f2-be8a-5ee03e2a9766\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving eicfull_b4a89_unrelaxed_rank_4_model_1.pdb to eicfull_b4a89_unrelaxed_rank_4_model_1.pdb\n",
            "User uploaded file \"eicfull_b4a89_unrelaxed_rank_4_model_1.pdb\" with length 720171 bytes\n"
          ]
        }
      ],
      "source": [
        "#@title Upload a PDB file\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  os.rename(fn, 'input.pdb')\n",
        "\n",
        "# Create project "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IoT3eYIblH6",
        "outputId": "e5ba94ad-077d-48f8-f6e4-c50f546b6c51",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/openawsem/mm_create_project.py': [Errno 2] No such file or directory\n",
            "cp: cannot stat 'input-openmmawsem.pdb': No such file or directory\n",
            "python3: can't open file '/content/./openawsem/helperFunctions/convertOpenmmTrajectoryToStandardMovie.py': [Errno 2] No such file or directory\n",
            "python3: can't open file '/content/mm_run.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "#@title Run a Molecular Dynamics with the [AWSEM](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008308) Coarse-Grained Force-Field for Proteins \n",
        "Temperature = 250.0 #@param {type:\"number\"}\n",
        "Timesteps   = 10000 #@param {type:\"number\"}\n",
        "\n",
        "!export OPENAWSEM_LOCATION=/content/openawsem/ && PATH={$PATH}:/content/stride_loc && python3 /content/openawsem/mm_create_project.py input.pdb\n",
        "!cp input-openmmawsem.pdb template.pdb\n",
        "!export OPENAWSEM_LOCATION=/content/openawsem/ && python3 ./openawsem/helperFunctions/convertOpenmmTrajectoryToStandardMovie.py template.pdb\n",
        "# can we only report on every 10,000 frames.. or add status bar?\n",
        "!export OPENAWSEM_LOCATION=/content/openawsem/ && python3 mm_run.py input  --steps $Timesteps --tempStart $Temperature --tempEnd $Temperature -f forces_setup.py \n",
        "\n",
        "#@markdown ---\n",
        "#@markdown > Lu, Wei, et al. \"OpenAWSEM with Open3SPN2: A fast, flexible, and accessible framework for large-scale coarse-grained biomolecular simulations.\" <em>PLoS computational biology 17.2 (2021): e1008308 </em>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Analysis Tools\n",
        "%%capture\n",
        "!pip install MDAnalysis\n",
        "!git clone https://github.com/Dan-Burns/molecular_dynamics_analysis_tools.git\n",
        "!pip install paramagpy # not using atm\n",
        "!pip install biopandas\n",
        "!pip install scipy\n",
        "!pip install parmed\n",
        "from molecular_dynamics_analysis_tools.rdcs import *\n",
        "from molecular_dynamics_analysis_tools.rdcs_least_squares import *\n",
        "from molecular_dynamics_analysis_tools.useful_functions import identical_subunits\n",
        "#### for rebuilding sidechains from CG structures\n",
        "#https://download.igb.uci.edu/sidepro_readme.txt\n",
        "!wget https://download.igb.uci.edu/sidepro.linux.tar.gz\n",
        "!tar xvf sidepro.linux.tar.gz\n",
        "!conda install -c conda-forge pdbfixer\n",
        "####\n",
        "# might need this because using wget can throw an error randomly\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "###\n",
        "\n",
        "import py3Dmol\n",
        "from sklearn.decomposition import PCA\n",
        "import mdtraj as md\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "claXqoq4_HTX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSLREcYwACYm",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "## Silence errors/warnings\n",
        "#%%capture\n",
        "\n",
        "#@title ##Perform RMSD Clustering of the Trajectory to Generate Conformational Ensembles\n",
        "from Bio.PDB import PDBParser\n",
        "import MDAnalysis.analysis.encore as encore\n",
        "import MDAnalysis as mda\n",
        "from MDAnalysis.analysis import align\n",
        "\n",
        "#@markdown Set Maximum Number of Clusters to Generate.\n",
        "#@markdown This will produce n ensembles composed of 1 to n structures.\n",
        "#@markdown Subsequent RDC fitting will reveal the best ensemble among these.\n",
        "n_clusters = 5 #@param {type:\"number\"}\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pdb   = 'template.pdb'\n",
        "model = PDBParser().get_structure('structure',pdb) \n",
        "\n",
        "# hold chain IDs and residue object lists\n",
        "# use to compare the subunits and figure out if they're identical or not\n",
        "chains = {chain.id: chain.get_unpacked_list() for chain in model.get_chains()}\n",
        "\n",
        "trajectory = 'movie.dcd'\n",
        "structure = 'template.pdb'\n",
        "u = mda.Universe(structure, trajectory)\n",
        "\n",
        "#############################\n",
        "# if it's a homo-multimer, separate the subunits and concatenate into\n",
        "# a long single-subunit trajectory for clustering\n",
        "# Otherwise deal with the trajectory and one or more subunits in its original form\n",
        "if identical_subunits(chains) == True: \n",
        "  # make a dictionary of chain/subunit keys with atom selection values\n",
        "  selections = {}\n",
        "  for chain in chains.keys():\n",
        "    selections[chain] = u.select_atoms('segid '+chain)\n",
        "\n",
        "  # make a directory to hold the seperated subunits\n",
        "  out_dir = 'seperated_trajectories/'\n",
        "  if not os.path.exists(out_dir):\n",
        "      os.makedirs(out_dir)\n",
        "  # clean the directory if it already exists\n",
        "  for f in os.listdir(out_dir):\n",
        "      os.remove(out_dir+f)\n",
        "\n",
        "  # Instead of writing the seperated trajectories to files, \n",
        "  # can probably use u.merge() - but people might want to download trajectories...\n",
        "  for chain, selection in selections.items():\n",
        "      with mda.Writer(f'{out_dir}chain_{chain}.dcd', selection.n_atoms) as W:\n",
        "          for ts in u.trajectory:\n",
        "              W.write(selection)\n",
        "  # write one subunit to a pdb\n",
        "  # can probably just use the selection instead of a saved pdb\n",
        "  # assuming we're just working with EI dimer for this\n",
        "  selections[list(selections.keys())[0]].write('one_subunit.pdb')\n",
        " \n",
        "  # align the seperated subunit trajectory\n",
        "  # conatenate\n",
        "  seperated_trajs = [out_dir+traj for traj in os.listdir(out_dir) if traj.endswith('dcd')]\n",
        "  pdb = 'one_subunit.pdb'\n",
        "  ref = mda.Universe(pdb)\n",
        "  sep_u = mda.Universe(pdb, seperated_trajs)\n",
        "  aligned_traj_name = 'aligned_seperated_subunits.dcd'\n",
        "  print('Aligning trajectory.')\n",
        "  align.AlignTraj(sep_u, ref,select='name CA',filename=aligned_traj_name).run()\n",
        "\n",
        "\n",
        "else: ## just align the original trajectory for clustering \n",
        "  from MDAnalysis.analysis import align\n",
        "  print('Aligning trajectory.')\n",
        "  aligned_traj_name = 'aligned_traj.dcd'\n",
        "  ref = mda.Universe(pdb)\n",
        "  align.AlignTraj(u, ref, select='name CA',filename=aligned_traj_name).run()\n",
        "\n",
        "for cluster_iteration in range(n_clusters):\n",
        "    print(f\"Cluster iteration {cluster_iteration+1}\")\n",
        "    structure = pdb\n",
        "    trajectory = aligned_traj_name\n",
        "    u = mda.Universe(structure, trajectory)\n",
        "    # add n_jobs argument so that all n_init are run in parallel (default is 10)\n",
        "    ensemble = encore.cluster(u, method=encore.clustering.ClusteringMethod.KMeans(n_clusters=cluster_iteration+1))\n",
        "    #####################################\n",
        "    # change this selection from 'name CA' to 'protein' when using structure file with \n",
        "    # correct residue names\n",
        "    #####################################\n",
        "    selection = u.select_atoms('protein')\n",
        "    ######################################\n",
        "    clusters = []\n",
        "    for i, cluster in enumerate(ensemble.clusters):\n",
        "        u.trajectory[cluster.centroid]\n",
        "        if not os.path.exists(f'cluster_{cluster_iteration+1}/'):\n",
        "          os.makedirs(f'cluster_{cluster_iteration+1}/')\n",
        "        selection.write(f'cluster_{cluster_iteration+1}/centroid_{i+1}.pdb')\n",
        "        clusters.append(int(cluster.centroid))\n",
        "\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Nonlinear Least Squares Fitting of RDCs to Ensembles\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "uploaded_rdcs = files.upload()\n",
        "\n",
        "for fn in uploaded_rdcs.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded_rdcs[fn])))\n",
        "  os.rename(fn, 'rdcs.dat')\n",
        "#@markdown Upload your RDC file\n",
        "\n",
        "rdc_file = 'rdcs.dat'\n",
        "\n",
        "rdc_results = 'rdc_fitting_results'\n",
        "if os.path.exists(rdc_results):\n",
        "  pass\n",
        "else:\n",
        "  os.makedirs(rdc_results)\n",
        "\n",
        "\n",
        "# make a list of the folders that contain the cluster iteration centroids\n",
        "# will have to change the list comprehension if we add option to do specific cluster iterations\n",
        "cluster_folders = [f'cluster_{i+1}' for i in range(n_clusters)]\n",
        "fig, axs = plt.subplots(n_clusters,1,\n",
        "                        sharex=True,sharey=True,\n",
        "                        figsize=(8,n_clusters*8))\n",
        "for i, pdb_folder in enumerate(cluster_folders):\n",
        "  print(f'Performing fit on ensemble {i+1}')\n",
        "  fit = FitRDCs(pdb_folder,rdc_file)\n",
        "  fit.fit_rdcs()\n",
        "  df = pd.DataFrame({'resids':fit.residue_ids, 'RDCs':fit.fitted_rdcs})\n",
        "  df.to_csv(f'{rdc_results}/{i+1}_member_ensemble_rdcs.csv')\n",
        "  # save R to text file\n",
        "  with open(f\"{rdc_results}/{i+1}_member_ensemble_R_factor.txt\",'w') as f:\n",
        "    f.write(str(round(fit.Rfactor,4)))\n",
        "  # add saving residuals and final par\n",
        "\n",
        "  # plot\n",
        "  axs.flat[i].plot(fit.rdc_values, fit.fitted_rdcs, marker='o', lw=0, ms=2, c='r',\n",
        "          label=f'R = {round(fit.Rfactor,4)}')\n",
        "\n",
        "  # plot the diagonal\n",
        "  l, h = axs.flat[i].get_xlim()\n",
        "  axs.flat[i].plot([l,h],[l,h],'-k', zorder=0, ms=2)\n",
        "  axs.flat[i].set_xlim(l,h)\n",
        "  axs.flat[i].set_ylim(l,h)\n",
        "\n",
        "  # labels\n",
        "  axs.flat[i].set_xlabel(\"Experimental RDCs\")\n",
        "  axs.flat[i].set_ylabel(f\"{i+1} Member Ensemble RDCs\")\n",
        "  axs.flat[i].legend()\n",
        "fig.savefig(f'{rdc_results}/rdc_plots.pdf')\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "WXu7nzFUuUz2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and Download the Results\n",
        "#@markdown If the results archive does not download automatically (possibly due to an adblocker), click on the little folder icon to the left, navigate to the `seq_to_ensemble_output.zip` file, right-click (or click on the adjacent three dots) and select \\\"Download\\\".\n",
        "# add sidechains back to the coarse grained centroid structures\n",
        "\n",
        "print(\"Please wait a few moments while the protein sidechains are rebuilt on the coarse grained structures.\\n\\\n",
        "        Keep in mind that the sidechain conformations are unlikely to be accurate.\")\n",
        "        \n",
        "from pdbfixer import PDBFixer\n",
        "from openmm.app import PDBFile\n",
        "\n",
        "cluster_folders = [f'cluster_{i+1}' for i in range(n_clusters)]\n",
        "# just enumerating this to track progress\n",
        "for count, cluster_folder in enumerate(cluster_folders):\n",
        "  print(f'Rebuilding sidechains for ensemble {count+1}',flush=True)\n",
        "  #make a new folder to hold the structures with sidechains\n",
        "  if os.path.exists(f'all_atom_{cluster_folder}'):\n",
        "    pass\n",
        "  else:\n",
        "    os.makedirs(f'all_atom_{cluster_folder}')\n",
        "  for pdb_index, cg_pdb in enumerate([f'{cluster_folder}/centroid_{i+1}.pdb' \n",
        "                         for i in range(len(os.listdir(cluster_folder)))]):\n",
        "    fixer = PDBFixer(filename=cg_pdb)\n",
        "    fixer.findMissingResidues()\n",
        "    fixer.findMissingAtoms()\n",
        "    ### TODO this will need to account for multi-chain structures, right now it's assuming there is just one \n",
        "    # chain with terminals that need to be fixed\n",
        "    # get the first and last residues of a single chain\n",
        "    first, last = list(fixer.missingAtoms.keys())[0], list(fixer.missingAtoms.keys())[-1]\n",
        "    # new dictionary with only the terminal N and C atoms \n",
        "    # this is because sidepro needs all of the backbone atoms, otherwise the terminal residues will be deleted\n",
        "    to_fix = {first:[atom for atom in fixer.missingAtoms[first] if atom.name=='N'], \n",
        "              last:[atom for atom in fixer.missingAtoms[last] if atom.name=='C']} \n",
        "    fixer.missingAtoms = to_fix\n",
        "    # add the missing terminal N and C atoms\n",
        "    fixer.addMissingAtoms()\n",
        "    # just hold the fixed terminal residue pdb and delete after conversion\n",
        "    PDBFile.writeFile(fixer.topology, fixer.positions, open('temp.pdb', 'w'))\n",
        "    # sidepro2.0 is super fast, pdbfixer takes too long to add all of the sidechains (2+ minutes - can't do this for dozens of structures)\n",
        "    # sending to /dev/null since this produces a ton of output\n",
        "    !cd SIDEpro2.0/; ./SIDEpro -i ../temp.pdb -f ../cleaned_pdbs/input.pdb -o ../all_atom_{cluster_folder}/sc_centroid_{pdb_index +1}.pdb &> /dev/null\n",
        "    !rm temp.pdb\n",
        "print('Zipping everything up.')\n",
        "!zip -FSr \"seq_to_ensemble_output.zip\" all_atom_cluster_* rdc_fitting_results &> /dev/null\n",
        "files.download(\"seq_to_ensemble_output.zip\")\n",
        "print('Done')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gqenuSbNI-fC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ColabOpenAWSEM.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}